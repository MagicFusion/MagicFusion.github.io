
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@400" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-iYQeCzEYFbKjA/T2uDLTpkwGzCiq6soy8tYaI1GyVh/UjpbCx/TYkiZhlZB6+fzT" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="styles.css" />

    <title>MagicFusion</title>
  </head>
  <body>
    <div style="padding: 2rem 0; background-color: #fff;">
<!--       <h1 style="text-align: center;">MagicFusion</h1> -->
<!--       <h1 style="text-align: center;"><img style="width: 15%" src='./static/logo.png'></h1>  -->
      <h2 style="text-align: center;">MagicFusion: Boosting Text-to-Image Generation Performance</h2>
      <h2 style="text-align: center;">by Fusing Diffusion Models(ICCV 2023)</h2>
      <h2 style="text-align: center;">Accepted by ICCV 2023</h2>
    </div>
    
    
    
<!--     <div class="container" style="max-width: 840px;">
                <div class="row">
                    <div class="col-md author"><a href="https://chenhsuanlin.bitbucket.io/" target="_blank">Chen-Hsuan Lin</a>*</div>
                    <div class="col-md author"><a href="https://www.cs.toronto.edu/~jungao/" target="_blank">Jun Gao</a>*</div>
                    <div class="col-md author"><a href="http://lumingtang.info" target="_blank">Luming Tang</a>*</div>
                    <div class="col-md author"><a href="https://tovacinni.github.io" target="_blank">Towaki Takikawa</a>*</div>
                    <div class="col-md author"><a href="https://www.cs.utoronto.ca/~xiaohui/" target="_blank">Xiaohui Zeng</a>*</div>
                </div>

                <div class="row">
                    <div class="col-md author"><a href="https://xunhuang.me" target="_blank">Xun Huang</a></div>
                    <div class="col-md author"><a href="https://karstenkreis.github.io/" target="_blank">Karsten Kreis</a></div>
                    <div class="col-md author"><a href="https://www.cs.utoronto.ca/~fidler/" target="_blank">Sanja Fidler</a><sup>†</sup></div>
                    <div class="col-md author"><a href="http://mingyuliu.net/" target="_blank">Ming-Yu Liu</a><sup>†</sup></div>
                    <div class="col-md author"><a href="https://tsungyilin.info" target="_blank">Tsung-Yi Lin</a></div>
                </div>

                <div class="row" style="margin-top:0.5rem; margin-left: auto; margin-right: auto;">
                    <div class="col-md">
                        <h9>*<sup>†</sup> : equal contributions</h9>
                        <div id="affiliation">NVIDIA Corporation</div>
                    </div>
                </div>
    </div>
 -->
    <div class="authors">
      <p style="padding-bottom: 5px;">Jing Zhao<sup>1</sup> , Heliang Zheng<sup>2</sup>, Chaoyue Wang<sup>2</sup>, Long Lan<sup>1</sup>, Wenjing Yang<sup>1†</sup></p>
      <p class="smaller">National University of Defense Technology<sup>1</sup>; JD Explore Academy<sup>2</sup></p> 
    </div> 
    <div align="center" style="text-align: center; padding: 20px 100px; background-color: #fff;">
<!--       <embed src="./static/figure1_final.pdf" type="application/pdf" width="100%" height="100%" internalinstanceid="81 /> -->
      <object type="image/jpeg" data="./static/figure1.png" width=1200px height="100%" ></object>
    </div>
    
    <div class="authors" style="text-align: center!important; font-family: 'Noto Sans', sans-serif; margin:30px">
      <span class="link-block" style="font-style: inherit;font-weight: inherit;background-color: #dee2e6; padding: 10px 30px; border-radius: 25px;text-align: center;margin-right: 20px;">
         <a href="https://arxiv.org/abs/2303.13126" target="_blank"
                  class="external-link button is-normal is-rounded">
                  <span class="icon">
                    <i class="fas fa-book-reader"></i>
                  </span>
                  <span>Paper</span>
                </a>
      </span>
      <span class="link-block" style="font-style: inherit;font-weight: inherit;background-color: #dee2e6; padding: 10px 30px; border-radius: 25px;text-align: center;">
                <a href="https://github.com/MagicFusion/MagicFusion.github.io" target="_blank"
                class="external-link button is-normal is-rounded">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
              </span>
    </div>
   

    <!-- <div class="topgallery">
      <div class="input-output">
        <img src="./static/srn_shapenet/cars_6_cond.png">
        <span>&#8594</span>
        <video autoplay loop muted><source type="video/mp4" src="./static/srn_shapenet/cars_6_hyp.mp4"></video>
      </div>
      <div class="input-output">
        <img src="./static/srn_shapenet/chairs_17_cond.png">
        <span>&#8594</span>
        <video autoplay loop muted><source type="video/mp4" src="./static/srn_shapenet/chairs_17_hyp.mp4"></video>
      </div>
      <div class="input-output">
        <img src="./static/srn_shapenet/cars_42_cond.png">
        <span>&#8594</span>
        <video autoplay loop muted><source type="video/mp4" src="./static/srn_shapenet/cars_42_hyp.mp4"></video>
      </div>
      <div class="input-output">
        <img src="./static/srn_shapenet/chairs_121_cond.png">
        <span>&#8594</span>
        <video autoplay loop muted><source type="video/mp4" src="./static/srn_shapenet/chairs_121_hyp.mp4"></video>
      </div>
    </div> -->

    <div class="abstract">
      <div class="inside">
        <h3 style="text-align: center;">Abstract</h2>
        <p class="text">
The advent of open-source AI communities has produced a cornucopia of powerful text-guided diffusion models that are trained on various datasets. While few explorations have been conducted on ensembling such models to combine their strengths. In this work, we propose a simple yet effective method called Saliency-aware Noise Blending (SNB) that can empower the fused text-guided diffusion models to achieve more controllable generation. Specifically, we experimentally find that the responses of classifier-free guidance are highly related to the saliency of generated images. Thus we propose to trust different models in their areas of expertise by blending the predicted noises of two diffusion models in a saliency-aware manner. SNB is training-free and can be completed within a DDIM sampling process. Additionally, it can automatically align the semantics of two noise spaces without requiring additional annotations such as masks. Extensive experiments show  the impressive effectiveness of SNB in various applications.
        </p>
<!--         <br>
        <br> 
        <a class="read-paper" href="https://arxiv.org/pdf/2211.14108.pdf" target="_blank"><button>Research Paper</button></a>   -->
      </div>
    </div>
    
    <div style="padding: 50px;background-color: rgb(230, 230, 230);color:black; text-align: center;">
      <h2>An overview of our Saiency-aware Noise Blending.</h2>
    </div>
     <div style="text-align: center; padding:50px 100px; background-color: #fff; font-size:20px">
        <!-- <h3 style="text-align: center;"></h5> -->
      <video autoplay loop muted style="max-width: 60vw;"><source type="video/mp4" src="./static/method_show2.mp4"></video>
       <p style="text-align: center;">An overview of our Saiency-aware Noise Blending. Given two diffusion models, we first design a ''Noise to salience map'' module to  obtain salience maps. After that, we can generate saliency-aware masks based on the salience maps. Finally, we blend the diffusion models in the noise space according to the mask. (*) classifier-free guidances are noises instead of noisy images, and we add the image here just for visualization.</p>
    </div>
    
    
    <div style="padding: 50px;background-color: rgb(230, 230, 230);color:black; text-align: center;">
      <h2>Fine-grained Fusion</h2>
    </div>
    <div style="text-align: center; padding: 0; background-color: #fff;">
      <video autoplay loop muted style="max-width: 60vw;"><source type="video/mp4" src="./static/car_show3.mp4"></video>
    </div>
    
    <div style="padding: 50px;background-color: rgb(230, 230, 230);color:black; text-align: center;">
      <h2>Recontextualization</h2>
    </div>
    <div style="text-align: center; padding: 0; background-color: #fff;">
      <video autoplay loop muted style="max-width: 60vw;"><source type="video/mp4" src="./static/dreambooth_show.mp4"></video>
    </div>
    
    <div style="padding: 50px;background-color: rgb(230, 230, 230);color:black; text-align: center;">
      <h2>Cross-domain Fusion</h2>
    </div>
    <div style="text-align: center; padding: 0; background-color: #fff;">
      <video autoplay loop muted style="max-width: 60vw;"><source type="video/mp4" src="./static/cartoon_show2.mp4"></video>
    </div>
    
    <div style="padding: 50px;background-color: rgb(230, 230, 230);color:black; text-align: center;">
      <h2>Displaying more experimental results.</h2>
    </div>
    <div align="center" style="text-align: center; padding: 20px 100px; background-color: #fff;">
      <h3 style="text-align:center;padding: 20px; margin-top: 20px;">Fine-grained Fusion</h3>
      <object type="image/jpeg" data="./static/car_show.jpg" width="90%" height="80%" ></object>
       <h3 style="text-align:center;padding: 20px; margin-top: 20px;">Recontextualization</h3>
      <object type="image/jpeg" data="./static/dreambooth_show.jpg" width="90%" height="80%" ></object>
       <h3 style="text-align:center;padding: 20px; margin-top: 20px;">Cross-domain Fusion</h3>
      <object type="image/png" data="./static/homepage_cartoon.png" width="90%" height="80%" ></object>
    </div>
    
   

<!--    <div class="white">
      <figure class="sampler">
         <div align="center"><object type="image/jpeg" data="./static/method.jpg" width="90%" height="80%" ></object>
        </div>
       <figcaption><p> xxx</p></figcaption>
      </figure>

    </div>  -->
<!--     <div class="container">
            <div id="citation">
                waiting。。。。。
            </div>
     </div> -->


    <!-- <div class="authors">
      <p style="padding-bottom: 25px;">Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, Mohammad Norouzi</p>
      <p class="smaller">Google Research</p>
    </div> -->

<!--     <div class="thanks">
      <h1>Special Thanks</h1>
      <br>
      <p class="smaller" style="text-align: justify;">We would like to thank Ben Poole for thoroughly reviewing this work, and providing useful feedback and ideas since the earliest stages of our research. We thank Tim Salimans for providing us with stable code to train diffusion models, which we used as the starting point for this paper, as well as code for neural network modules and diffusion sampling tricks used in their more recent "Video Diffusion Models" paper. We thank Erica Moreira for her critical support on juggling resource allocations for us to execute our work. We also thank David Fleet for his key support on securing the computational resources required for our work, as well as the many helpful research discussions throughout. We additionally would like to acknowledge and thank Kai-En Lin and Vincent Sitzmann for providing us with the outputs of their work on novel view synthesis and their helpful correspondence. We thank Mehdi Sajjadi and Etienne Pot for consistently lending us their expertise, especially on issues with datasets, cameras, rays, and all-things 3D. We thank Keunhong Park, who refactored a lot of the NeRF code we used, which made it easier to implement our proposed 3D consistency evaluation scheme. We thank Sarah Laszlo for helping us ensure our models and datasets meet responsible AI practices. Finally, we'd like to thank Geoffrey Hinton, Chitwan Saharia, and more widely the Google Brain Toronto team for their useful feedback, suggestions, and ideas throughout our research effort.</p>
    </div> -->

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-u1OknCvxWvY5kfmNBILK2hRnQC3Pr17a+RTT6rIHI7NnikvbZlHgTPOOmMi466C8" crossorigin="anonymous"></script>
  </body>
</html>
